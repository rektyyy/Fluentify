services:
  nginx:
    image: nginx:alpine
    ports:
      - "5000:80"
    volumes:
      - ./web-app/build:/usr/share/nginx/html
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - llm
      - tts
  llm:
    image: ghcr.io/huggingface/text-generation-inference:2.4.0
    ports:
      - "8080:8080"
    environment:
      - MODEL_ID=unsloth/Llama-3.2-1B-Instruct
      - PORT=8080
      # - QUANTIZE=awq
      # - MAX_INPUT_LEN=4096
      # - MAX_TOTAL_TOKENS=4096
      # - MAX_BATCH_PREFILL_TOKENS=4096
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
  tts:
    image: ghcr.io/coqui-ai/xtts-streaming-server:latest-cuda121
    ports:
      - "8000:80"
    environment:
      - COQUI_TOS_AGREED=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
